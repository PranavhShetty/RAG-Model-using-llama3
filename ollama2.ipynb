{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\prana\\AppData\\Local\\Temp\\ipykernel_30056\\1086761679.py:4: LangChainDeprecationWarning: The class `Ollama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import OllamaLLM``.\n",
      "  model = Ollama(model=\"llama2\")\n",
      "C:\\Users\\prana\\AppData\\Local\\Temp\\ipykernel_30056\\1086761679.py:5: LangChainDeprecationWarning: The class `OllamaEmbeddings` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import OllamaEmbeddings``.\n",
      "  embeddings = OllamaEmbeddings()\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "\n",
    "model = Ollama(model=\"llama2\")\n",
    "embeddings = OllamaEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "I'm just an AI, I don't have feelings or emotions like humans do, so I can't really experience the world in the same way that you do. However, I'm here to help answer any questions you may have and provide information on a wide range of topics. Is there something specific you would like to know or discuss?\n"
     ]
    }
   ],
   "source": [
    "response = model.invoke(\"How are you?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'data/Yang_Semiconductor_Defect_Detection_by_Hybrid_Classical-Quantum_Deep_Learning_CVPR_2022_paper.pdf', 'page': 0}, page_content='Abstract  \\n \\nWith the rapid development of artificial intelligence and \\nautonomous driving technology, the demand for \\nsemiconductors is projected to rise substantially. However, \\nthe massive expansion of semiconductor manufacturing \\nand the development of new technology will bring many \\ndefect wafers. If these defect wafers have not been co rrectly \\ninspected, the ineffective semiconductor processing on \\nthese defect wafers will cause additional impact to our \\nenvironment , such as excessive carbon dioxide emission \\nand energy consumption.  \\nIn this pape r, we utilize the information processing \\nadvantages of quantum computing to promote the defect learning defect review (DLDR). We propose a classical -\\nquantum hybrid algorithm for deep learning on near -term \\nquantum processors. By tuning parameters implemente d \\non it, quantum circuit driven by our framework learns a \\ngiven DLDR task, include of wafer defect map \\nclassification, defect pattern classification, and hotspot detection. In addition, we explore parametrized quantum \\ncircuits with different expressibility  and entangling \\ncapacities. These results can be used to build a future \\nroadmap to develop circuit -based quantum deep learning \\nfor semiconductor defect detection.  \\n \\n1. Introduction \\nThe industry presents a paradox. The achievement of \\nglobal climate goals will d epend on semiconductors. They \\nare an integral part of solar arrays, wind turbines, and \\nelectric vehicles. But chip manufacturing requires huge \\namounts of energy and water, and it emit a lot of exhaust \\ngas during the production process. In addition to switc hing \\nto renewables, chipmakers could also implement \\nefficiencies in fabs to reduce their carbon footprint, \\nincluding ineffective processing on the defect wafer.  \\nIn the semiconductor manufacturing, there are three \\nmain  types of defects: wafer defect map, defect pattern, and \\nhotspot. Wafer defect map is used to visualize the \\ndistribution of defect patterns and identify potential process and tool issues. Continuous -monitoring wafer defect maps are crucial for yield manage ment because a \\nsudden increase in the problematic patterns can be feedback to operation engineers to resolve the related issue. \\nGenerally, it is known that conventional defect patterns \\nsuch as cluster, scratch, and ring are closely related to a \\ncertain typ e of process. For example, as shown in Figure 1, \\nthe defect map “Edge” is caused by the damaged low \\nthermal mass (LTM) pad. LTM is an industrial grade self -\\nregulating heating cable used for pipelines and chambers. Under the high temperature and high pressure of chemical \\nvapor deposition (CVD) process, LTM pad will gradually \\nget aged and cracked. Then, it will produce particles that \\nfall on the edge of wafer surface. Another example “Cluster” \\nis a mass of particles on the wafer surface during the Etching pro cess. This defect is usually caused by the aged \\ncomponent of side O -ring. When this defect maps are \\ndetected, it means that the chamber needs to maintenance and replace aging parts with new ones.  \\nThe defect pattern is the unit that composes the wafer \\nmap, which image is obtained by a higher resolution \\ninspection tool. Each defect pattern signals the root cause, and engineers can diagnose the failure and prevent it from happening again. As shown in Figure 2, the defect pattern \\n\"Multi- dots\" mainly occurs in t he WET Etching chamber \\npipeline and is caused by the dirty nozzles of xFlow. Another example is “Fallon” that often occurs on the CVD chamber. The main reason is the fall on particle caused by \\nthe damage of the point -of-use (POU) filter. The damage \\ncompone nt must be replaced immediately to avoid this \\ndefect from happening again.   \\nSemiconductor Defect Detection by Hybrid Classical -Quantum Deep Learning  \\n \\n \\n Yuan -Fu Yang  Min Sun  \\n National Tsing Hua University  National Tsing Hua University  \\n 101, Section 2, Kuang -Fu Road, 101, Section 2, Kuang -Fu Road,'),\n",
       " Document(metadata={'source': 'data/Yang_Semiconductor_Defect_Detection_by_Hybrid_Classical-Quantum_Deep_Learning_CVPR_2022_paper.pdf', 'page': 0}, page_content='Yuan -Fu Yang  Min Sun  \\n National Tsing Hua University  National Tsing Hua University  \\n 101, Section 2, Kuang -Fu Road, 101, Section 2, Kuang -Fu Road, \\n Hsinchu, Taiwan R.O.C.  Hsinchu, Taiwan R.O.C.  \\n yfyangd@gmail.com summin@ee.nthu.edu.tw  \\n \\n \\nFigure 1: Defect map and its root cause.  \\n2323'),\n",
       " Document(metadata={'source': 'data/Yang_Semiconductor_Defect_Detection_by_Hybrid_Classical-Quantum_Deep_Learning_CVPR_2022_paper.pdf', 'page': 1}, page_content='A hotspot is the region of mask layout patterns where \\nfailures including open and short c ircuits are more likely to \\nhappen during semiconductor manufacturing. Hotspot \\ndetection is used to check potential circuit failures at the post optical proximity correction (OPC) stage when \\ntransferring designed patterns onto silicon wafers. Figure -\\n3 show the example of Epitaxy (EPI) hotspot and its root \\ncause. We can find that this wafer has the layout pattern of SiGe wavy. This hotspot usually caused by tungsten loss \\n(W loss) in the CVD process. In addition, the lithography (LIT) hotspot has received much  attention in the recent \\nyears. LIT hotspots caused by light diffraction during manufacturing procedure due to a substantial mismatch \\nbetween lithography wavelength and semiconductor \\ntechnology feature size.  \\nDeep learning as a hot computer vision technolog y has \\nbeen actively realized in many fields. Although it has been serving the semiconductor industry in many areas, the \\ncontribution of deep learning in semiconductor defect \\ndetection is tremendous. However, deep learning algorithms tend to give probabilis tic results and contain \\ncorrelated components but at the same time suffer computational bottlenecks due to the curse of dimensionality. Similar to deep learning, quantum \\ncomputing (QC) provides probabilistic results based on \\nmeasurements formed by intrinsi cally coupled quantum \\nsystems. QC can provide potentially exponential speedups \\ndue to their ability to perform massively parallel \\ncomputations on the superposition of quantum states.  \\nIn addition, each artificial neuron is usually constructed \\nby linear conn ected layers, with non -linear activation \\nfunctions connected at the end. In this work, we replace the linear part by the quantum circuit to take advantage of \\npossible speedup in quantum computation.  We propose the \\nhybrid classical -quantum deep learning (H CQDL) based \\non quantum circuit for the above various types of defect \\ninspection. Our HCQDL consists of classical and quantum \\nlayers. The classical layers implemented by the \\ncombination of self -proliferation and self -attention block. \\nSelf-proliferation bloc k used a series of linear \\ntransformations to generate more feature maps at a cheaper \\ncomputing cost. Then, self -attention block learned a wealth \\nof information about long- range dependencies from this \\ngenerated feature.  \\nThe quantum layer implemented by various quantum \\ncircuit built in the continuous -variable architecture, which \\nencodes quantum information in continuous degrees of freedom such as the amplitudes of the electromagnetic field. \\nBy tuning parameters implemented on it, quantum circuit \\ndriven by  our framework learns a given DLDR task, \\ninclude of wafer defect map classification, defect pattern \\nclassification, and hotspot detection.  The main \\ncontributions of the paper are:  \\n(1) We introduce a new network architecture, Self -\\nProliferation -and-Attentio n block (SP&A Block), which \\ncan perform feature engineering in a more efficient way.  \\n(2) We present the parametrized quantum circuit (PQC) \\nwith different expressibility and entangling capacities, and compare their training performance to quantify the \\nexpe cted benefits.  \\n(3) We provide a future roadmap to develop circuit -\\nbased hybrid quantum -classical deep learning for \\nsemiconductor defect detection.  \\nThe rest of the paper has been organized as follows: \\nSection 2 describes the related works of deep learning defect review (DLDR) and quantum machine learning. \\nSection 3 presents our proposed HCQDL model and \\nexplore it in detail theoretically. Section 4 compares the \\nresults of latest DLDR models and our proposed method \\naccording to different metrics through severa l experiments. \\nFinally, we conclude with some final thoughts and give \\nfuture work recommendations in section 5 . \\n2. Related works  \\nIn this section, we briefly discuss DLDR and quantum \\nmachine learning backgrounds.'),\n",
       " Document(metadata={'source': 'data/Yang_Semiconductor_Defect_Detection_by_Hybrid_Classical-Quantum_Deep_Learning_CVPR_2022_paper.pdf', 'page': 1}, page_content='future work recommendations in section 5 . \\n2. Related works  \\nIn this section, we briefly discuss DLDR and quantum \\nmachine learning backgrounds.  \\n2.1. Defect learning defect review  \\nWafer Defect Map . Wafer defect map is used to visualize \\ndefect patterns, identify potential process issues, and provide yield engineers with vital information to help them \\nidentify the root cause of die failures during semiconductor \\nmanufacturing processes. Nakata et al. [1] proposed the big \\ndata analysis enables comprehensive and long -term \\nmonitoring automation. They make use of fast and scalable \\nmethods of clustering and pattern mining and realize daily  \\nFigure 2: Defect pattern and its root cause  \\n \\nFigure 3: Hotspot and its root cause . \\n2324'),\n",
       " Document(metadata={'source': 'data/Yang_Semiconductor_Defect_Detection_by_Hybrid_Classical-Quantum_Deep_Learning_CVPR_2022_paper.pdf', 'page': 2}, page_content='comprehensive monitoring with massive manufacturing \\ndata. They also apply deep learning to classification of \\nwafer failure map patterns. Their deep learning model was \\na retrainable, one- class classifier with five layers whose \\nparameters were determined empirically. They used to train it on the frequently occurring pattern,  employing \\nunlabeled images to monitor if that defect pattern emerges \\nagain. Nakazawa and Kulkarni [2] proposed the \\nconvolutional neural network (CNN) for wafer map defect \\nclassification and retrieval of similar maps. They \\ndemonstrate that by using only synthetic data for network \\ntraining, real wafer maps can be classified with high \\naccuracy. Kyeong and Kim [3] proposed a mixed defect classification system consisting of four CNN models for \\nfour basic defect types of the dataset. The proposed model \\nwas able to identify 16 defect types as combinations of four \\nbasic types. Kim et al. [4] presented a neural network -\\nbased bin coloring method and built a four -layered CNN to \\ndistinguish good and bad wafers. However, they did not classify the defect wafers into thei r respective defect types, \\nwhich is a necessary step to analyze the root cause of the \\ndefects. di Bella et al. [5] adopted submanifold sparse CNN \\nas a binary classifier for sparse data of larger images. They also used oversampling to overcome class imbalan ce, such \\nas rotations, flips, and noise injection. Kong and Ni [6] \\nproposed a multi -step detection system for multi- defect \\nwafers classification. First, a binary CNN is used to classify wafers with overlapping and non -overlapping \\npatterns. The wafer maps w ith a single pattern or non -\\noverlapping mixed -type pattern were segmented into \\nsingle pattern maps and then classified by a CNN. In the \\nlatest research, deep learning architecture has become the \\nmainstream [8 -10] on wafer defect map classification task. \\nWe plan to combine the advantages of deep learning and \\nquantum computing as a roadmap for future research.  \\nDefect Pattern . The defect pattern is the unit that \\ncomposes the wafer map. It is the image obtained by a \\nhigher resolution inspection tool. Defect pat tern \\nrecognition is suited to deep learning, a powerful \\nsupervised learning technique that does not need manual feature design. Beuth et al. [11] used a biological visual \\nattention method to perform the deep learning defect \\nreview. This biological visual a ttention mechanism is \\nmainly to realize the boundary search of the defect chip by \\nsimulating the signal of the pre -frontal cortex (PFC). \\nHowever, the attention mechanism of this study has two \\nproblems: 1. The PFC of this model is only applicable to \\nthe search of geometric shapes, and not for textures. 2. The \\nmodel cannot adaptively learn the attention area of \\nattention. Therefore, how to develop an adaptive attention \\nmechanism (self -attention) for defect pattern classification \\nis the direction of future res earch. Chen et al. [12] proposed \\na lightweight network called WDD -Net. The method refers \\nto MobileNet- V2, using depthwise separable convolutions \\nto reduce parameters and calculations, The experimental results show that the detection speed of WDD -Net is 5 \\ntimes faster than that of VGG -16. Yang and Sun [13] \\nproposed a new DLDR architecture, named self -\\nproliferating neural network (SPNet), which can \\neffectively generate more feature maps with a lower \\ncomputational cost.  \\nHotspot . Hotspot detection is defined as  the procedure of \\nfinding the hotspots from the layout that would cause \\nprintability issues during lithograph. F. Yang et al. [14] \\nused support vector machine (SVM) as the lithography \\nhotspot classifier. They adopted spectral clustering for \\nfeature extract ion, and their accuracy reached 95.66% on \\nthe public dataset ICCAD -2012. V. S. Ajna and N. George \\n[15] proposed a method of layout hot spot detection based \\non deep learning. This method achieves 93% accuracy with'),\n",
       " Document(metadata={'source': 'data/Yang_Semiconductor_Defect_Detection_by_Hybrid_Classical-Quantum_Deep_Learning_CVPR_2022_paper.pdf', 'page': 2}, page_content='the public dataset ICCAD -2012. V. S. Ajna and N. George \\n[15] proposed a method of layout hot spot detection based \\non deep learning. This method achieves 93% accuracy with \\nthe least number of false alarms. In subseq uent research, \\ndeep learning became the main model architecture of hotspot detection [16- 18]. X. Huang et al. [19] proposed an \\nensemble deep learning based on multiple sub -models. \\nThis method achieves recall rate 98.8% on the ICCAD -\\n2012.  \\nQuantum computatio n is a paradigm that furthermore \\nincludes nonclassical effects such as superposition, interference, and entanglement, giving it potential \\nadvantages over classical computing models. Therefore, \\nwe introduce the application of quantum machine learning in the  next section, hoping to improve the weakness of \\nclassical deep learning by using quantum computing.  \\n2.2. Quantum machine learning  \\nRecent developments in quantum computing allowed \\nscientists to look at computational problems from a new \\nperspective. Researchers have been investigating quantum \\ncomputing tools for a computational advantage for the deep learning problem. As near -term quantum devices move \\nbeyond the point of classical stimulability, also known as \\nquantum supremacy [20], it is of utmost importance to \\ndiscover new applications for noisy intermediate scale \\nquantum (NISQ) devices [21] which are expected to be \\navailable in the next few years. Among the most promising \\napplications for quantum computing is Quantum Machine \\nLearning (QML) [22 -23]. Recent advan ces in QML have \\nbeen dominated by a class of algorithms called hybrid \\nquantum -classical variational algorithms. Sasaki and \\nCarlini [24] have delved into the notion of semi -classical \\nand universal strategies whereby, in the former, classical methods are adapted to work on quantum systems whereas \\nin the latter the methods are purely quantum in nature. A. \\nAjagekar and F. You [25] proposed the fault diagnosis deep learning method based on quantum computing. This \\nmethod enjoys superior performance with an average fault \\ndiagnosis rate of 80% and tremendously low false alarm \\nrates for the monitoring of Tennessee Eastman process.  \\nIn this paper, we present quantum computing based deep  \\n2325'),\n",
       " Document(metadata={'source': 'data/Yang_Semiconductor_Defect_Detection_by_Hybrid_Classical-Quantum_Deep_Learning_CVPR_2022_paper.pdf', 'page': 3}, page_content='learning methods for semiconductor defect detection that \\nare capable of overcoming the computational challenges \\nfaced by conventional techniques performed on classical \\ncomputers. The proposed model effectively detects defect \\npattern by leveraging the superior feature extraction of self -\\nproliferation and self -attention to facilitate prope r \\nclassification between normal and defect wafers. Then, a quantum computing assisted generative training process \\nfollowed by supervised discriminative training is used to \\ntrain this model.  \\n3. Method  \\nOur technique, called hybrid classical -quantum deep \\nlearning (HCQDL), uses quantum circuits to nonlinearly \\ntransform classical inputs into features that can then be used in a number of deep learning algorithms. HCQDL consists of classical la yer, quantum layer, and fully \\nconnection layer (as shown in Figure 4). The classical layer \\nis implemented by the self -proliferation and self -attention \\n(SP&A) block that used to extract feature maps effectively. \\nThe quantum layer is composed of the quantum circuit that \\ncan generate highly complex kernels whose calculations could be classically intractable. The fully connection layer \\nis implemented by the non -linear activation functions to \\ncalculate the probability of various semiconductor defect. \\nTo demonstr ate the power of the proposed method, we \\npresent results of using HCQDL to generate eigenstates of simple molecules, complex entangled ground states, and ground states of the transverse Hamiltonian with varying \\nlocal fields and spin -spin couplings. The approach \\nachieved high  accurate results and can be generalized for \\ncreating quantum states of complex systems.  \\n3.1. Classical Layer  \\nThe classical layer is mainly composed of two parts: \\nself-proliferation block and self -attention block. Self -\\nproliferation block use d a series of linear transformations \\nto generate more feature maps at a cheaper cost.  Self-\\nattention block learned a wealth of information about long - range dependencies from this generated feature maps . \\nSelf-Proliferation Block.  Traditional convolution is  a \\nseries of convolution operations to increase the feature \\ndepth. Self -proliferation block generates the same number \\nof features through linear transform. The process is similar to DNA unzipping and replication, which can effectively \\nincrease the number o f features. The process of Self -\\nproliferation is as follows (as shown in Figure 5).  \\nThe first step of self -proliferation is a small amount of \\nclassical  convolution which can be built upon a \\ntransformation 𝑓𝑓∈ℝ\\n𝑐𝑐×𝑘𝑘×𝑘𝑘×𝑛𝑛 mapping an input 𝑀𝑀∈\\nℝ𝑐𝑐×ℎ×𝑤𝑤 to feature maps 𝑀𝑀′∈ℝℎ′×𝑤𝑤′×𝑛𝑛: \\n𝑀𝑀′=𝑀𝑀⊗𝑓𝑓 (1) \\nwhere ⊗ denotes convolution,  𝑐𝑐 and 𝑛𝑛 is the number of \\ninput and out channels, ℎ and ℎ′ is the height of the input \\nand out data , 𝑤𝑤 and 𝑤𝑤′ is the width of the input and out \\ndata.  we can obtain n feature maps by classical convolution.  \\nThe second step is a cheap operation, represented by  ρ \\nin the Eq. 2. This is a linear transformation that uses depth -\\nwise convolution  to further obtain n  feature map s \\naccording to the following functi on: \\n𝑢𝑢𝑖𝑖,𝑗𝑗=𝜌𝜌𝑖𝑖,𝑗𝑗(𝑚𝑚𝑖𝑖′),∀𝑖𝑖=1,…,𝑠𝑠,𝑗𝑗=1,…,𝑡𝑡 (2) \\nWhere 𝑚𝑚𝑖𝑖′ is the i -th unit of feature map 𝑀𝑀′ generated by \\nEq. 1., 𝜌𝜌𝑖𝑖,𝑗𝑗 is the j -th linear transformation for generating \\nthe j-th feature map 𝑢𝑢𝑖𝑖,𝑗𝑗. If 12 feature maps are generated, \\nself-proliferation can generate 6 of them (as shown in \\nFigure 5). Therefore, the computation cost can be reduced \\nby half.  \\nSelf-Attention Block.  The self -attention block aims at \\nstrengthening the features of the query position via \\naggregating information from other positions. The basic architecture formulated as:  \\n𝑦𝑦\\n𝑖𝑖=𝑥𝑥𝑖𝑖+𝑤𝑤𝑒𝑒2𝑅𝑅𝑅𝑅𝑅𝑅𝑅𝑅 (𝑅𝑅𝐿𝐿�𝑤𝑤𝑒𝑒1�𝑅𝑅𝑤𝑤𝑔𝑔𝑥𝑥𝑗𝑗\\n∑𝑅𝑅𝑤𝑤𝑔𝑔𝑥𝑥𝑚𝑚 ℎ×𝑤𝑤\\n𝑚𝑚=1ℎ×𝑤𝑤\\n𝑗𝑗=1�) (3) \\nWe denote 𝑥𝑥𝑖𝑖 as the feature map of on input instance \\n(e.g., defect pattern), where h  and w is the height and width \\nof input x . 𝑤𝑤𝑒𝑒1 and 𝑤𝑤𝑒𝑒2 denote linear transform matrices'),\n",
       " Document(metadata={'source': 'data/Yang_Semiconductor_Defect_Detection_by_Hybrid_Classical-Quantum_Deep_Learning_CVPR_2022_paper.pdf', 'page': 3}, page_content='𝑗𝑗=1�) (3) \\nWe denote 𝑥𝑥𝑖𝑖 as the feature map of on input instance \\n(e.g., defect pattern), where h  and w is the height and width \\nof input x . 𝑤𝑤𝑒𝑒1 and 𝑤𝑤𝑒𝑒2 denote linear transform matrices \\n(1×1 convolution) which is used to bottleneck transform.  \\n𝑤𝑤𝑔𝑔 is the weight for global attention pooling. LN  denotes   \\nFigure 4: Architecture of hybrid classical -quantum deep learning . \\n \\nFigure 5: Self -proliferation process . \\n2326'),\n",
       " Document(metadata={'source': 'data/Yang_Semiconductor_Defect_Detection_by_Hybrid_Classical-Quantum_Deep_Learning_CVPR_2022_paper.pdf', 'page': 4}, page_content='the layer  normalization, which is utilized to filter the \\nredundant information and refine the obtained contextual \\ninformation. Specifically, our self -attention block consists \\nof 3 parts (as shown in Figure 6): (a) global attention \\npooling f or context modeling; (b) bottleneck transform to \\ncapture channel -wise dependencies; and (c) denotes the \\nfusion function to broadcast element -wise addition for \\nfeature fusion. This function can aggregate the global \\ncontext features to the features of each p osition [3 0]. \\nSelf-Proliferation -and- Attention Block.  It consists of 4 \\nparts (as shown in Figure 7): (a) expansion layer used self -\\nproliferation block to increase input dimension to generate \\nmore feature maps at a cheaper cost. (b) depthwise convolution ar e used for feature extraction and self -\\nattention are used to capture long -range dependencies. (c) \\ncompression layer used to reduce feature dimension to be \\nthe same as input feature. Then, we can transform spatial \\ninformation with low computation. Finally, in (d) we refer \\nto MobileNetV2 [26] to build inverted residuals. The \\nresiduals architecture was first proposed by He et al. in ResNet [27]. Its design concept is to learn residuals on the \\nnetwork to avoid gradient vanishing problems. ResNet\\'s \\narchitecture is to compress first then expand. MobileNet\\'s \\narchitecture is the opposite of ResNet that expand first then \\ncompress. The idea of MobileNet is to \"capture features in \\nhigh dimensions and transfer information in low \\ndimensions\".  3.2. Quantum Layer  \\nThe quantum la yer implemented by various quantum \\ncircuit built in the continuous -variable architecture. It \\nconsists of three consecutive parts (as shown in Figure 8). An encoding circuit encodes classical data to states of the \\nqubits followed by a parametrized quantum c ircuit (PQC) \\nthat is applied to transform these states to their optimal \\nlocation on the Hilbert space (as shown in Figure 9). Finally, measure the output of PQC along the z -axis with \\nthe 𝜎𝜎\\n𝑧𝑧 operator.  \\n3.2.1 Encoding Circuit  \\nThe encoding circuit is used to encode the classical data \\ninto the physical states of Hilbert space [28] for quantum computing. In this paper, we applied 3 encoding methods, \\nincluding basis encoding, amplitude encoding, and angle \\nencoding.  \\nBasis Encoding.  In basis encoding, the data has to be in \\nform of a binary string to get encoding. Approximating a scalar value to its binary form and then transforming it to a quantum state. For example, if we have a classical dataset \\ncontaining two examples \\n𝑥𝑥𝑘𝑘=01 and 𝑥𝑥𝑘𝑘+1=11, the \\ncorresponding quan tum state after basis encoding is  |𝑥𝑥𝑘𝑘⟩=\\n|01⟩ and |𝑥𝑥𝑘𝑘+1⟩=|11⟩. In short, Basis encoding encodes \\nan n-bit binary string 𝑥𝑥𝑘𝑘 to an n -qubit quantum state as:  \\n|𝑥𝑥𝑘𝑘⟩=|𝑖𝑖𝑥𝑥⟩ (4) \\nwhere |𝑖𝑖𝑥𝑥⟩ is a computational basis state, and every binary \\nstring has a unique integer repres entation 𝑖𝑖𝑥𝑥=∑ 2𝑘𝑘𝑥𝑥𝑘𝑘𝑛𝑛−1\\n𝑘𝑘=0 . \\nAmplitude Encoding.  The amplitude encoding is also \\nknown as wave function embedding. The amplitude is the \\nheight of a wave. In the amplitude encoding, the data points \\nare transformed into amplitudes of the quantum state. A \\nnormalized classical N -dimensional 𝑥𝑥𝑘𝑘 is represent ed by \\nthe amplitudes of a n -qubit quantum state |𝑥𝑥𝑘𝑘⟩ as: \\n|𝑥𝑥𝑘𝑘⟩=�𝑥𝑥𝑘𝑘|𝑖𝑖𝑥𝑥⟩𝑁𝑁\\n𝑘𝑘=0(5) \\nwhere N is the length of vector 𝑥𝑥𝑘𝑘 into amplitudes of an n -\\nqubit quantum state with 𝑛𝑛=𝑙𝑙𝑙𝑙𝑙𝑙2(𝐿𝐿). {|𝑖𝑖𝑥𝑥⟩} is the \\ncomputational basis for the Hilbert space. Since the  \\nFigure 6: Architecture of self -attention.  \\n \\nFigure 7: Architecture of self -proliferation -and-attention block.  Self-Proliferate \\nBlock\\nDWConv\\nSelf-Proliferate \\nBlockBatch Norm.\\nReLU\\nBatch Norm.(a) Expansion Layer\\n(c) Compression Layer(b) Convolution & Self -Attention\\n(d) Inverted Residuals Self-Attention\\nBlock\\n+ \\nFigure 8: An example of q uantum circuit with 4 qubits. Each \\nqubit uses the rotation gate  𝑅𝑅(𝜃𝜃) by the angle θ  around x, y, z. \\nCNOT  gate are used for every 2 qubits in order.  \\n2327'),\n",
       " Document(metadata={'source': 'data/Yang_Semiconductor_Defect_Detection_by_Hybrid_Classical-Quantum_Deep_Learning_CVPR_2022_paper.pdf', 'page': 5}, page_content='classical information forms the amplitudes of a quantum \\nstate, the input needs to satisfy the normalization condition: \\n|𝑥𝑥|2=1. \\nAngle Encoding.   Angle encoding makes use of rotation \\ngates to encode classical information 𝑥𝑥𝑘𝑘∈ℝ𝑁𝑁 without any \\nnormalization condition. The classical information determines angles of rotation gates:  \\n|𝑥𝑥\\n𝑘𝑘⟩=⊗𝑘𝑘=0𝑁𝑁𝑅𝑅(𝑥𝑥𝑘𝑘)|0𝑁𝑁⟩ (6) \\nWhere N is the number of qubits, R  can be one of 𝑅𝑅𝑥𝑥, 𝑅𝑅𝑦𝑦, \\n𝑅𝑅𝑧𝑧. Usually,  N used for encoding is equal to the dimension \\nof vector 𝑥𝑥𝑘𝑘. Since it is 2 π-periodic one may want to limit \\nℝ𝑁𝑁 to the hypercube [0,2𝜋𝜋]⊗𝑛𝑛. The i -th feature 𝑥𝑥𝑘𝑘 is \\nencoded into the k -th qubit via a Pauli- X rotation.  Figure 9 \\nshow the angle encoding wi th a rotation angle of θ  around \\nz-axis on the Hilbert space.  After activation function \\n𝑡𝑡𝑡𝑡𝑛𝑛ℎ(𝑥𝑥), the output 𝑥𝑥𝑘𝑘∈[−1, 1] from the end of classical \\nlayer. Then, the rotation angle is mapped between 𝜃𝜃∈\\n[0,𝜋𝜋] due to the periodicity of the cosine function . This is \\nrelevant since the expectation value is taken with respect to \\nthe 𝜎𝜎𝑧𝑧 operator  at the end of the circuit execution.  \\n3.2.2 Parametrized Quantum Circuit  \\nA parameterized quantum circuit is a quantum circuit \\nconsisting of parameterized gates with fixed depth. These circuits have free parameters: the rotation angle of the \\nquantum state. We use quantum  circuits and repeat \\nmultiple times with different random p arameters, instead \\nof using the large and complex classical neural networks. The parameterized quantum circuit also consists of one-qubit gates as well as Controlled Not ( CNOT ). Some more \\ncomplicated gates may also be used in PQC which can be decomposed in to one qubit gates and CNOT . In general, an \\nn qubits PQC can be written as:  \\n𝑢𝑢(𝜃𝜃�)|𝜑𝜑⟩=��𝑢𝑢\\n𝑖𝑖𝑚𝑚\\n𝑖𝑖=1�|𝜑𝜑⟩ (7) \\nwhere 𝑢𝑢(𝜃𝜃�) is the set of unitary gates and m  is the number \\nof quantum gate. 𝜃𝜃� is the set of parameters { 𝜃𝜃0, 𝜃𝜃1,... 𝜃𝜃𝑘𝑘}, \\nwhere k is the total number of parameters and |𝜑𝜑⟩ is the \\ninitial quantum state after data encoding. The unitary gate taking parameters is rotation gate 𝑅𝑅 �𝜃𝜃��, given by:  \\n𝑅𝑅𝑥𝑥(𝜃𝜃�)=𝑅𝑅−𝑖𝑖𝜃𝜃�\\n2𝜎𝜎𝑥𝑥,𝑅𝑅𝑦𝑦(𝜃𝜃�)=𝑅𝑅−𝑖𝑖𝜃𝜃�\\n2𝜎𝜎𝑦𝑦,𝑅𝑅𝑧𝑧(𝜃𝜃�)=𝑅𝑅−𝑖𝑖𝜃𝜃�\\n2𝜎𝜎𝑧𝑧(8) \\nwhere {𝜎𝜎𝑥𝑥,𝜎𝜎𝑦𝑦,𝜎𝜎𝑧𝑧} is Paul i matrices. The operation of 𝑢𝑢 can \\nbe modified by changing parameters 𝜃𝜃�. Thus, the output \\nstate can be optimized to approximate the wanted state by \\nchanging parameter 𝜃𝜃�. By optimizing the parameters used \\nin 𝑢𝑢(𝜃𝜃�), PQC approximates the wanted quantum st ates. To \\nachieve better entanglement of the qubits before appending \\nnonlinear operations, the n  qubits PQC has n  repeated \\nlayers in our model. By optimizing the parameters, the \\ngeneral PQC tries to approximate arbitrary states so that it \\ncan be used for di fferent specific molecules.  \\nIn order to provide computational speedup by \\norchestrating constructive and destructive interference of \\nthe amplitudes in quantum computing, we constructed m \\nrotation gates 𝑅𝑅𝑥𝑥 on the n qubits PQC as our basic quantum \\ncircuit, which can be written as:  \\n� (⊗𝑗𝑗=0𝑚𝑚𝑅𝑅𝑥𝑥(𝜃𝜃𝑖𝑖+𝑛𝑛×𝑗𝑗) 𝐶𝐶𝐿𝐿𝐶𝐶𝐶𝐶𝑖𝑖,𝑖𝑖+1)𝑛𝑛\\n𝑖𝑖=1(9) \\nwhere 𝑅𝑅𝑥𝑥 represents the unitary gate of rotation -𝑥𝑥. 𝜃𝜃𝑖𝑖+𝑛𝑛×𝑗𝑗 \\nis adjustable parameter of unitary gates. 𝐶𝐶𝐿𝐿𝐶𝐶𝐶𝐶𝑖𝑖,𝑖𝑖+1 \\nrepresents 𝐶𝐶𝐿𝐿𝐶𝐶𝐶𝐶 gate with 𝑚𝑚 as the control qubit, and 𝑛𝑛 is \\nthe total number of qubits. Figure 10 show the basic \\nquantum circuit with n=4 and m=3. Each qubit uses the \\nrotation gate 𝑅𝑅(𝜃𝜃) by the angle 𝜃𝜃 around x-axis on the \\nHilbert space , and  𝐶𝐶𝐿𝐿𝐶𝐶𝐶𝐶 gate is used for every 2 qubits.  \\n3.3. Fully Connection Layer  \\nAfter obtaining all features from PQC, we feed them into \\na fully connection (FC) layer. We use the softmax activation function, so the output of FC  layer will be a \\nprobability distribution. The i -th element of the output is \\nthe probability that this data point belongs to the i -th \\ncategory, and we predict that this data point belongs to the \\ncategory with the highest probability. In order to predict the actual label, we calculate the cumulative dis tance'),\n",
       " Document(metadata={'source': 'data/Yang_Semiconductor_Defect_Detection_by_Hybrid_Classical-Quantum_Deep_Learning_CVPR_2022_paper.pdf', 'page': 5}, page_content='category, and we predict that this data point belongs to the \\ncategory with the highest probability. In order to predict the actual label, we calculate the cumulative dis tance \\nbetween the predicted label and the actual label as the loss function to be optimized:  \\nℒ(𝜔𝜔\\n1,𝑏𝑏1,𝜃𝜃,𝜔𝜔2,𝑏𝑏2)=−1\\n𝐿𝐿��𝑦𝑦𝑗𝑗𝑖𝑖𝑙𝑙𝑙𝑙𝑙𝑙𝑦𝑦�𝑗𝑗𝑖𝑖𝐾𝐾\\n𝑗𝑗=1𝑁𝑁𝑖𝑖=1(10)  \\nFigure 9: Angle encoding with a rotation angle of θ around z-axis \\non the Hilbert space . \\n \\nFigure 10: Basic quantum circuit with n= 4, m=3  \\n2328'),\n",
       " Document(metadata={'source': 'data/Yang_Semiconductor_Defect_Detection_by_Hybrid_Classical-Quantum_Deep_Learning_CVPR_2022_paper.pdf', 'page': 6}, page_content='where 𝜔𝜔1,𝑏𝑏1 is the parameter of classical layer. 𝜃𝜃 is the \\nparameter of quantum layer. 𝜔𝜔2,𝑏𝑏2 is the parameter of FC \\nlayer. Perform the above procedure enough times to get a \\ngood estimate of the expected value of 𝑦𝑦�𝑗𝑗𝑖𝑖 with minimum \\nloss function.  \\n4. Results  \\n4.1. Experiment C ircuits \\nIn deep learning, it is very useful to transform data into \\na higher -dimensional feature space. In the same way, there \\nare two strategies for using quantum circuits to generate \\nhigher dimensional features: entangling more and more \\nqubits. However, quantum computers are now in their \\ninfancy, the available qubits are limited. In this experiment, \\nwe adopt different entangling strategies to verify our \\nmethod on the 4 qubits circuit s ystem.  In addition to the \\nbasic quantum circuit (as shown in Figure -10), we also \\nchose Circuit -5/6/16/17 provided by Sim et al. [29], which \\nhas better expressive ability and entanglement ability, as the parametrized quantum circuit (as shown in Figure 11).  \\nTable 1: Ablation Study of Defect Pattern Classification  \\n4.2. Ablation Study  \\nTo demonstrate the usefulness of  quantum circuit, our \\nmodel had been verified on two industry data sets, defect pattern and EPI hotspot. We performed a rigorous ablation study and showed the quantitative comparison in Table 1. \\nWe refer to ResNet50, SENet, MobileNetV3 and our \\nproposed model as the basic model to integrate with \\nvarious quantum circuits. In addition to the basic circuit we proposed, we added the Circuit -5/6/16/17 presented in the \\nprevious section. We also adopted four encoding strategy: \\nbasic, amplitude, and angle encoding.  \\nAblation results yielded many significant findings. First, \\nCircuit 5 and Circuit 6 is a fully connected graph Model  Classical \\nLayer  Quantum Layer  \\nFeature  \\nExtraction  Encoding  Circuit  \\nBasic  5 6 16 17 \\nHybrid  ResNet50  \\n[27] Basic  93.73  95.42  95.03  94.42  93.94  \\nAmplitude  93.74  95.43  95.04  94.43  93.95  \\nAngle  93.81  95.50  95.11  94.51  94.02  \\nClassical  - 92.76  \\nHybrid  SENet  \\n[45] Basic  95.09  96.75  96.43  96.04  95.85  \\nAmplitude  95.27  96.94  96.61  96.22  96.03  \\nAngle  95.42  97.08  96.75  96.36  96.17  \\nClassical  - 93.55  \\nHybrid  MobileNetV3  \\n[31] Basic  92.93  94.13  93.70  93.24  93.40  \\nAmplitude  93.02  94.21  93.79  93.33  93.49  \\nAngle  93.39  94.58  94.15  93.70  93.86  \\nClassical  - 92.09  \\nHybrid  SP&A -Net \\n(proposed \\nmodel)  Basic  95.92  97.85  97.05  96.54  96.11  \\nAmplitude  96.06  97.99  97.19  96.68  96.25  \\nAngle  96.55  98.47  97.68  97.17  96.73  \\nClassical  - 94.27   \\nFigure 11: Quantum Circuit by Sim et al. [ 29]. \\n \\nFigure 12: ResNet50 [27] with Different S trategy  of Quantum \\nCircuit for Defect Pattern Classification  \\n \\nFigure 13: SENet [ 45] with Different S trategy  of Quantum \\nCircuit for EPI Hotspot Classification  \\n2329'),\n",
       " Document(metadata={'source': 'data/Yang_Semiconductor_Defect_Detection_by_Hybrid_Classical-Quantum_Deep_Learning_CVPR_2022_paper.pdf', 'page': 7}, page_content='arrangement of qubits which led to both favorable \\nexpressibility and entangling capability. Therefore, the \\nnetwork model with Circuit 5 and Circui t 6 has high \\naccuracy regardless of semiconductor defect detect task (as \\nshown in Figure 12 and Figure 13). Second, Circuit 5 and \\nCircuit 16 with controlled Z -rotation ( CRz) gates is better \\nthan Circuit 6 and 17, respectively. It because that CRz \\noperation s in the entangling block commute with each \\nother and thus the effective unitary operation comprised of CRz gates can be expressed using unique generator terms \\nthat are fewer than the number of parameters for these gates. Finally, the basic circuit without any contr olled rotation \\ngate has the lowest accuracy among the 4 classification \\ntasks. This suggests that, if one is trying to design a PQC \\nto increase expressibility, it is better to insert single -qubit \\ngates which skew the controlled gate rotation axis away \\nfrom the control axis.  \\n4.3. Experiment Results  \\nWe compare our proposed model on the public dataset \\nof LIT hotspot (ICCAD -2012) and defect wafer map (WM-\\n811K) with the state -of-art method in the last 4 years. Table \\n2 and table 3 shows that our proposed model (HCQDL) has \\na higher test accuracy, which shows the advantages of \\nquantum computing. It mean that small quantum circuits \\nprovide significant performance lift over standard linear \\nclassical algorithms, improving classification accuracy \\nrates from arou nd 98% to 99.12%, and from around 96% \\nto 98.10% in these two public dataset, respectively.  \\nTable 2: Comparison of state -of-the-art methods on ICCAD -2012 \\nAuthors  Model  Feature \\nExtraction  Data  \\nAugmentation  Accuracy  \\nH. Yang, et al. \\n[32] CNN Feature Tensor \\nExtraction  Mirroring,  \\nFlipping  98.88%  \\nH. Yang, et al. \\n[33] CNN - Mirroring,  \\nFlipping,  \\nUpsampling  98.20%  \\nY. Tomioka, et al. \\n[34] Real AdaBoost  Histogram of \\nOriented Light \\nPropagation Rotating, \\nReflecting  99.01%  \\nF. Yang, et al. \\n[35] Efficient  \\nSVM  Spectral  \\nClustering  - 95.66%  \\nV. Borisov and J. Scheible [36]  CNN - Flipping,  \\nGaussian \\nfilter  98.94%  \\nH. Yang, et al. \\n[37] CNN - - 97.36%  \\nV.S. Ajna and N. \\nGeorge [15]  CNN - - 93.00%  \\nX. Huang, et al. \\n[19] Ensemble  \\nCNN   Combine \\nPhysical Features  - 98.80%  \\nX. Lin, et al. [38]  Heterogeneous \\nFederated \\nLearning  Feature Tensor \\nExtraction  - 97.90%  \\nOur Proposed Model  Hybrid \\nClassical-\\nQuantum CNN  Self-Proliferate  \\nSelf-Attention  Mirroring,  \\nFlipping, \\nRotating  99.12%  Table 3: Comparison of state -of-the-art methods on WM -811K  \\nAuthors  Model  Feature \\nExtraction  Data  \\nAugmentation  Accuracy  \\nT. Nakazawa and \\nD. V. Kulkarni, \\n[2] CNN - - 96.30%  \\nN. Yu, Q. Xu and \\nH. Wang [7]  CNN PCA  - 93.25%  \\nJ. Yu and J. Liu \\n[10] CNN AE+PCA  - 97.30%  \\nJ. Yu, X. Zheng \\nand J. Liu [39]  CNN AE - 95.13%  \\nJ. Yu [40]  SVM  AE - 89.50%  \\nT.-H. Tsai and \\nY.-C. Lee [8]  MobileNetV2  - AE 97.01%  \\nM. B. Alawieh et \\nal. [9]  CNN - AE 94.00%  \\nS. Kang [41]  CNN  - Rotation  92.13%  \\nU. Batool et al. \\n[42] CNN - Flipping, \\nBrightness  90.44%  \\nM. Saqlain et al. \\n[43] CNN - Rotation, \\nflipping, \\nshifting, \\nzooming  96.20%  \\nD. Kim and P. \\nKang [44]  CNN - - 96.40%  \\nOur Proposed Model  Hybrid \\nClassical-\\nQuantum CNN  Self-Proliferate  \\nSelf-Attention  Mirroring,  \\nFlipping, \\nRotating  98.10%  \\n5. Conclusion  \\nOur experiments in this work were designed to highlight \\nthe novelties introduced by the quantum deep learning: the \\ngeneralizability of convolutional layers inside a typical \\nCNN architecture, the ability to use this quantum circuit on \\nsemiconductor defect datasets, and the potential use of features introduced by the parametrized quantum circuit. \\nThe experimental results show that our framework \\noutperforms the existing deep learning techniques. In \\naddition, based on recent progress, we reasonably believes \\nthat quantum advantage is achievable within the next 5 -10 \\nyears. Quantum computers promise access to fast linear'),\n",
       " Document(metadata={'source': 'data/Yang_Semiconductor_Defect_Detection_by_Hybrid_Classical-Quantum_Deep_Learning_CVPR_2022_paper.pdf', 'page': 7}, page_content='addition, based on recent progress, we reasonably believes \\nthat quantum advantage is achievable within the next 5 -10 \\nyears. Quantum computers promise access to fast linear \\nalgebra processing capabilities which are in principle able \\nto deliver the polynomial speed -up that allows kernel \\nmethods to process big data without relying on \\napproximations and heuristics.  \\nMoreover, there is much to be learned about how to \\nselect these parameterized circuits, how to choose the random numbers to mix with the input data, and how to \\noptimize the number of qubits to get good performance. \\nMany of our existing deep lea rning algorithms could be \\ntranslated into hybrid classical -quantum deep learning, \\nallowing them take advantage of new properties like \\nentanglement, superposition and parallel computing. It’s only a matter of time before these algorithms are changing \\nthe wo rld. We hope this paper can be a demonstration that \\nquantum machine learning has the potential to provide \\nsatisfactory solutions for semiconductor defect detection.  \\n2330'),\n",
       " Document(metadata={'source': 'data/Yang_Semiconductor_Defect_Detection_by_Hybrid_Classical-Quantum_Deep_Learning_CVPR_2022_paper.pdf', 'page': 8}, page_content='References  \\n[1] K. Nakata, R. Orihara, Y. Mizuoka and K. Takagi, \"A \\nComprehensive Big -Data-Based Monitoring System for \\nYield Enhancement in Semiconductor Manufacturing,\" in \\nIEEE Transactions on Semiconductor Manufacturing , vol. \\n30, no. 4, pp. 339-344, Nov. 2017.  \\n[2] T. Nakazawa and D. V. Kulkarni, \"Wafer Map  Defect \\nPattern Classification and Image Retrieval Using \\nConvolutional Neural Network,\" in IEEE Transactions on \\nSemiconductor Manufacturing , vol. 31, no. 2, pp. 309- 314, \\nMay 2018.  \\n[3] K. Kyeong and H. Kim, \"Classification of mixed-type defect patterns in wafer  bin maps using convolutional neural \\nnetworks,\" in IEEE Transactions on Semiconductor \\nManufacturing, vol. 31, no. 3, pp. 395-402, Aug. 2018.  \\n[4] J. Kim, H. Kim, J. Park, K. Mo and P. Kang, \"Bin2Vec: A better wafer bin map coloring scheme for comprehensible \\nvisualization and effective bad wafer classification,\" Appl. \\nSci., vol. 9, no. 3, pp. 597, Feb. 2019.  \\n[5] R. di Bella, D. Carrera, B. Rossi, P. Fragneto and G. \\nBoracchi, \"Wafer defect map classification using sparse \\nconvolutional networks,\" Proc. Int. Conf. Image  Anal. \\nProcess ., pp. 125 -136, 2019.  \\n[6] Y. Kong and D. Ni, \"Recognition and location of mixed-type \\npatterns in wafer bin maps,\"  Proc. IEEE Int. Conf. Smart \\nManuf. Ind. Logistics Eng. (SMILE), pp. 4 -8, Apr. 2019.  \\n[7] N. Yu, Q. Xu  and H. Wang, \"Wafer defect pattern \\nrecognition and analysis based on convolutional neural \\nnetwork,\" in IEEE Transactions on Semiconductor \\nManufacturing, vol. 32, no. 4, pp. 566-573, Nov. 2019.  \\n[8] T.-H. Tsai and Y. -C. Lee, \"A light -weight neural network \\nfor w afer map classification based on data augmentation,\" in \\nIEEE Transactions on Semiconductor Manufacturing , vol. \\n33, no. 4, pp. 663-672, Nov. 2020.  \\n[9] M. B. Alawieh, D. Boning and D. Z. Pan, \"Wafer map defect \\npatterns classification using deep selective learnin g,\" Proc. \\n57th ACM/IEEE Design Automat. Conf. (DAC) , pp. 1 -6, Jul. \\n2020.  \\n[10] J. Yu and J. Liu, \"Two -dimensional principal component \\nanalysis -based convolutional autoencoder for wafer map \\ndefect detection,\" IEEE Trans. Ind. Electron., vol. 68, no. 9, \\npp. 8789 -8797, Sep. 2021.  \\n[11] F. Beuth, T. Schlosser, M. Friedrich and D. Kowerko, \\n\"Improving automated visual fault detection by combining a \\nbiologically plausible model of visual attention with deep \\nlearning,\" Proc. 46th Annu. Conf. IEEE Ind. Electron. Soc. \\n(IECON), p p. 5323- 5330, Oct. 2020.  \\n[12] X. Chen et al., \"A Light -Weighted CNN Model for Wafer \\nStructural Defect Detection,\" in IEEE Access , vol. 8, pp. \\n24006 -24018, 2020.  \\n[13] Y.F. Yang and M. Sun, \" A Novel Deep Learning Architecture for Global Defect Classification: Self -\\nProliferating Neural Network (SPNet),\" 2020 31th Annual \\nSEMI Advanced Semiconductor Manufacturing Conference \\n(ASMC) , 2020.  \\n[14] F. Yang, C. C. Chiang, X. Zeng and D. Zhou, \"Efficient \\nSVM- based hotspot detection using spectral clustering,\" \\n2017 IEEE International Symposium on Circuits and \\nSystems (ISCAS) , pp. 1 -4, 2017.  \\n[15] V. S. Ajna and N. George, \"Detection of Hotspots in Layout \\nPatterns using Deep Learning,\" 2019 10th International Conference on Computing, Communication and Networking \\nTechnologies (ICCCNT), pp. 1- 6, 2019.  \\n[16] K. Liu, B. Tan, G. R. Reddy, S. Garg, Y. Makris and R. Karri, \\n\"Bias Busters: Robustifying DL -based Lithographic Hotspot \\nDetectors Against Backdooring Attacks,\" in IEEE \\nTransactions on Computer -Aided Design of Integrated \\nCircuits and Systems, 2020.  \\n[17] T. Zhou et al., \"An effective method of contour extraction for SEM image based on DCNN,\" 2020 International \\nWorkshop on Advanced Patterning Solutions (IWAPS) , pp. \\n1-4, 2020.  \\n[18] T. Zhou et al., \"Mining Lithography Hotspots from Massive SEM Images Using Machine Learning Model,\" 2021 China \\nSemiconductor Technology International Conference \\n(CSTIC), pp. 1 -3, 2021.  \\n[19] X. Huang, R. Zhang, Y. Huang, P. Wang and M. Li,'),\n",
       " Document(metadata={'source': 'data/Yang_Semiconductor_Defect_Detection_by_Hybrid_Classical-Quantum_Deep_Learning_CVPR_2022_paper.pdf', 'page': 8}, page_content='Semiconductor Technology International Conference \\n(CSTIC), pp. 1 -3, 2021.  \\n[19] X. Huang, R. Zhang, Y. Huang, P. Wang and M. Li, \\n\"Enhancements of Model and Method in Lithography \\nHotspot Identification,\" 2021 Design, Automation & Test  in \\nEurope Conference & Exhibition (DATE) , pp. 102 -107, \\n2021.  \\n[20] S. Boixo, S. V. Isakov, V. N. Smelyanskiy, R. Babbush, N. Ding, Z. Jiang, et al., \"Characterizing quantum supremacy in near-term devices,\" Nature Phys. , vol. 14, no. 6, pp. 595-600, \\nJun. 2018.  \\n[21] J. Preskill, \"Quantum computing in the NISQ era and \\nbeyond,\" Quantum , vol. 2, pp. 79, Aug. 2018.  \\n[22] P. R. Giri and V. E. Korepin, \"A review on quantum search \\nalgorithms,\" Quantum Inf. Process. , vol. 16, no. 12, pp. 315, \\nNov. 2017.  \\n[23] T. M. Khan and A. Robles -Kell y, \"Machine Learning: \\nQuantum vs Classical,\" in  IEEE Access , vol. 8, pp. 219275 -\\n219294, 2020.  \\n[24] Masahide Sasaki and Alberto Carlini, \"Quantum learning \\nand universal quantum matching machine,\" APS Physical , \\nAug. 2020.  \\n[25] A. Ajagekar and F. You, \"Quantum computing assisted deep \\nlearning for fault detection and diagnosis in industrial \\nprocess systems,\" Computers & Chemical Engineering,  Vol. \\n143, Dec. 2020.  \\n[26] M. Sandler, A. Howard, M. Zhu, A. Zhmoginov and L. Chen, \\n\"MobileNetV2: Inverted Residuals and Linear Bottlenecks\", \\n2018 IEEE/CVF Computer Vision and Pattern Recognition Conference (CVPR) , pp. 4510- 4520, Jun. 2018.  \\n[27] K. He, X. Zhang, S. Ren and J. Sun, \"Deep Residual \\nLearning for Image Recognition\", 2016 IEEE Conference on \\nComputer Vision and Pattern Recognition (CVP R), pp. 770 -\\n778, Jun. 2016.  \\n[28] Maria Schuld, \"Supervised quantum machine learning models are kernel methods,\"  arXiv , Jan. 2021.  \\n[29] S. Sim, P. J. Johnson, et al., “Expressibility and entangling \\ncapability of parameterized quantum circuits for hybrid \\nquantum -class ical algorithms,” arXiv , 2019.  \\n[30] Y. Cao, J. Xu, S. Lin, F. Wei and H. Hu, “GCNet: Non -Local \\nNetworks Meet Squeeze-Excitation Networks and Beyond,” \\n2019 IEEE/CVF International Conference on Computer Vision Workshop (ICCVW) , pp. 1971 -1980, 2019.  \\n[31] A. Howard et a l., “Searching for MobileNetV3,” 2019 \\nIEEE/CVF International Conference on Computer Vision \\n(ICCV), pp. 1314- 1324, Nov. 2019.  \\n[32] H. Yang, J. Su, Y. Zou, B. Yu and E. F. Y. Young, \"Layout \\nHotspot Detection with Feature Tensor Generation and Deep \\n2331'),\n",
       " Document(metadata={'source': 'data/Yang_Semiconductor_Defect_Detection_by_Hybrid_Classical-Quantum_Deep_Learning_CVPR_2022_paper.pdf', 'page': 9}, page_content='Biased Learning ,\" Design Automation Conference (DAC) , \\n2017.  \\n[33] H. Yang, L. Luo, J. Su, C. Lin and B. Yu, \"Imbalance aware \\nlithography hotspot detection: a deep learning approach,\" \\nSPIE Advanced Lithography , 2017.  \\n[34] Y. Tomioka, T. Matsunawa, C. Kodama and S. Nojima, \"Lithograp hy hotspot detection by two-stage cascade \\nclassifier using histogram of oriented lieht propagation,\" 22nd ASP -DAC , 2017.  \\n[35] F. Yang, C. C. Chiang, X. Zeng and D. Zhou, \"Efficient \\nSVM- based hotspot detection using spectral clustering,\" \\n2017 IEEE International Symposium on Circuits and Systems (ISCAS) , pp. 1 -4, 2017.  \\n[36] V. Borisov and J. Scheible, \"Lithography Hotspots \\nDetection Using Deep Learning,\" 2018 15th International \\nConference on Synthesis, Modeling, Analysis and \\nSimulation Methods and Applications to Circuit Design (SMACD) , pp. 145-148, 2018.  \\n[37] H. Yang, Y. Lin, B. Yu and E. F. Y. Young, \"Lithography \\nhotspot detection: From shallow to deep learning,\" 2017 \\n30th IEEE International System -on-Chip Conference \\n(SOCC), pp. 233-238, 2017.  \\n[38] Xuezhong Lin, Jingyu Pan, Jinming Xu, Yiran Chen, Cheng Zhuo, \"Lithography Hotspot Detection via Heterogeneous \\nFederated Learning with Local Adaptation,\"  arXiv , Jul. 2021.  \\n[39] J. Yu, X. Zheng and J. Liu, \"Stacked convolutional sparse denoising auto-encoder for identification of defec t patterns \\nin semiconductor wafer map,\" Comput. Ind. , vol. 109, pp. \\n121-133, Aug. 2019.  \\n[40] J. Yu, \"Enhanced stacked denoising autoencoder -based \\nfeature learning for recognition of wafer map defects,\" in \\nIEEE Transactions on Semiconductor Manufacturing , vol. \\n32, no. 4, pp. 613 -624, Nov. 2019.  \\n[41] S. Kang, \"Rotation -invariant wafer map pattern \\nclassification with convolutional neural networks,\" IEEE \\nAccess , vol. 8, pp. 170650-170658, 2020.  \\n[42] U. Batool, M. I. Shapiai, H. Fauzi and J. X. Fong, \\n\"Convolutional neural netw ork for imbalanced data \\nclassification of silicon wafer defects,\" Proc. 16th IEEE Int. \\nColloq. Signal Process. Appl. (CSPA) , pp. 230 -235, Feb. \\n2020.  \\n[43] M. Saqlain, Q. Abbas and J. Y. Lee, \"A deep convolutional \\nneural network for wafer defect identification on an \\nimbalanced dataset in semiconductor manufacturing \\nprocesses,\" in IEEE Transactions on Semiconductor \\nManufacturing, vol. 33, no. 3, pp. 436-444, Aug. 2020.  \\n[44] D. Kim and P. Kang, \"Dynamic Clustering for Wafer Map \\nPatterns using Self -Supervised Learning on Convolutional \\nAutoencoders,\" in IEEE Transactions on Semiconductor \\nManufacturing, 2021.  \\n[45] J. Hu, L. Shen and G. Sun, “Squeeze-and -Excitation \\nNetworks,” 2018 IEEE /CVF Conference on Computer \\nVision and Pattern Recognition (CVPR) , pp. 7132-7141, Jun. \\n2018.  \\n2332')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader(\"data/Yang_Semiconductor_Defect_Detection_by_Hybrid_Classical-Quantum_Deep_Learning_CVPR_2022_paper.pdf\")\n",
    "pages = loader.load_and_split()\n",
    "pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: \n",
      "Answer the question based on the context below. If you can't \n",
      "answer the question, reply \"I don't know\".\n",
      "\n",
      "Context: Mary's sister is Susana\n",
      "\n",
      "Question: Who is Mary's sister?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "template = \"\"\"\n",
    "Answer the question based on the context below. If you can't \n",
    "answer the question, reply \"I don't know\".\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "prom = prompt.format(context=\"Mary's sister is Susana\", question=\"Who is Mary's sister?\")\n",
    "print(prom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'properties': {'context': {'title': 'Context', 'type': 'string'},\n",
       "  'question': {'title': 'Question', 'type': 'string'}},\n",
       " 'required': ['context', 'question'],\n",
       " 'title': 'PromptInput',\n",
       " 'type': 'object'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.input_schema.schema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Human: Hello Santiago! Your name is Santiago.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(\n",
    "    {\n",
    "        \"context\": \"The name I was given was Santiago\",\n",
    "        \"question\": \"What's my name?\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\prana\\Downloads\\llama\\llama-env\\lib\\site-packages\\pydantic\\_migration.py:283: UserWarning: `pydantic.error_wrappers:ValidationError` has been moved to `pydantic:ValidationError`.\n",
      "  warnings.warn(f'`{import_path}` has been moved to `{new_location}`.')\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.vectorstores import DocArrayInMemorySearch\n",
    "\n",
    "vectorstore = DocArrayInMemorySearch.from_documents(pages, embedding=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'data/Yang_Semiconductor_Defect_Detection_by_Hybrid_Classical-Quantum_Deep_Learning_CVPR_2022_paper.pdf', 'page': 7}, page_content='addition, based on recent progress, we reasonably believes \\nthat quantum advantage is achievable within the next 5 -10 \\nyears. Quantum computers promise access to fast linear \\nalgebra processing capabilities which are in principle able \\nto deliver the polynomial speed -up that allows kernel \\nmethods to process big data without relying on \\napproximations and heuristics.  \\nMoreover, there is much to be learned about how to \\nselect these parameterized circuits, how to choose the random numbers to mix with the input data, and how to \\noptimize the number of qubits to get good performance. \\nMany of our existing deep lea rning algorithms could be \\ntranslated into hybrid classical -quantum deep learning, \\nallowing them take advantage of new properties like \\nentanglement, superposition and parallel computing. It’s only a matter of time before these algorithms are changing \\nthe wo rld. We hope this paper can be a demonstration that \\nquantum machine learning has the potential to provide \\nsatisfactory solutions for semiconductor defect detection.  \\n2330'),\n",
       " Document(metadata={'source': 'data/Yang_Semiconductor_Defect_Detection_by_Hybrid_Classical-Quantum_Deep_Learning_CVPR_2022_paper.pdf', 'page': 5}, page_content='classical information forms the amplitudes of a quantum \\nstate, the input needs to satisfy the normalization condition: \\n|𝑥𝑥|2=1. \\nAngle Encoding.   Angle encoding makes use of rotation \\ngates to encode classical information 𝑥𝑥𝑘𝑘∈ℝ𝑁𝑁 without any \\nnormalization condition. The classical information determines angles of rotation gates:  \\n|𝑥𝑥\\n𝑘𝑘⟩=⊗𝑘𝑘=0𝑁𝑁𝑅𝑅(𝑥𝑥𝑘𝑘)|0𝑁𝑁⟩ (6) \\nWhere N is the number of qubits, R  can be one of 𝑅𝑅𝑥𝑥, 𝑅𝑅𝑦𝑦, \\n𝑅𝑅𝑧𝑧. Usually,  N used for encoding is equal to the dimension \\nof vector 𝑥𝑥𝑘𝑘. Since it is 2 π-periodic one may want to limit \\nℝ𝑁𝑁 to the hypercube [0,2𝜋𝜋]⊗𝑛𝑛. The i -th feature 𝑥𝑥𝑘𝑘 is \\nencoded into the k -th qubit via a Pauli- X rotation.  Figure 9 \\nshow the angle encoding wi th a rotation angle of θ  around \\nz-axis on the Hilbert space.  After activation function \\n𝑡𝑡𝑡𝑡𝑛𝑛ℎ(𝑥𝑥), the output 𝑥𝑥𝑘𝑘∈[−1, 1] from the end of classical \\nlayer. Then, the rotation angle is mapped between 𝜃𝜃∈\\n[0,𝜋𝜋] due to the periodicity of the cosine function . This is \\nrelevant since the expectation value is taken with respect to \\nthe 𝜎𝜎𝑧𝑧 operator  at the end of the circuit execution.  \\n3.2.2 Parametrized Quantum Circuit  \\nA parameterized quantum circuit is a quantum circuit \\nconsisting of parameterized gates with fixed depth. These circuits have free parameters: the rotation angle of the \\nquantum state. We use quantum  circuits and repeat \\nmultiple times with different random p arameters, instead \\nof using the large and complex classical neural networks. The parameterized quantum circuit also consists of one-qubit gates as well as Controlled Not ( CNOT ). Some more \\ncomplicated gates may also be used in PQC which can be decomposed in to one qubit gates and CNOT . In general, an \\nn qubits PQC can be written as:  \\n𝑢𝑢(𝜃𝜃�)|𝜑𝜑⟩=��𝑢𝑢\\n𝑖𝑖𝑚𝑚\\n𝑖𝑖=1�|𝜑𝜑⟩ (7) \\nwhere 𝑢𝑢(𝜃𝜃�) is the set of unitary gates and m  is the number \\nof quantum gate. 𝜃𝜃� is the set of parameters { 𝜃𝜃0, 𝜃𝜃1,... 𝜃𝜃𝑘𝑘}, \\nwhere k is the total number of parameters and |𝜑𝜑⟩ is the \\ninitial quantum state after data encoding. The unitary gate taking parameters is rotation gate 𝑅𝑅 �𝜃𝜃��, given by:  \\n𝑅𝑅𝑥𝑥(𝜃𝜃�)=𝑅𝑅−𝑖𝑖𝜃𝜃�\\n2𝜎𝜎𝑥𝑥,𝑅𝑅𝑦𝑦(𝜃𝜃�)=𝑅𝑅−𝑖𝑖𝜃𝜃�\\n2𝜎𝜎𝑦𝑦,𝑅𝑅𝑧𝑧(𝜃𝜃�)=𝑅𝑅−𝑖𝑖𝜃𝜃�\\n2𝜎𝜎𝑧𝑧(8) \\nwhere {𝜎𝜎𝑥𝑥,𝜎𝜎𝑦𝑦,𝜎𝜎𝑧𝑧} is Paul i matrices. The operation of 𝑢𝑢 can \\nbe modified by changing parameters 𝜃𝜃�. Thus, the output \\nstate can be optimized to approximate the wanted state by \\nchanging parameter 𝜃𝜃�. By optimizing the parameters used \\nin 𝑢𝑢(𝜃𝜃�), PQC approximates the wanted quantum st ates. To \\nachieve better entanglement of the qubits before appending \\nnonlinear operations, the n  qubits PQC has n  repeated \\nlayers in our model. By optimizing the parameters, the \\ngeneral PQC tries to approximate arbitrary states so that it \\ncan be used for di fferent specific molecules.  \\nIn order to provide computational speedup by \\norchestrating constructive and destructive interference of \\nthe amplitudes in quantum computing, we constructed m \\nrotation gates 𝑅𝑅𝑥𝑥 on the n qubits PQC as our basic quantum \\ncircuit, which can be written as:  \\n� (⊗𝑗𝑗=0𝑚𝑚𝑅𝑅𝑥𝑥(𝜃𝜃𝑖𝑖+𝑛𝑛×𝑗𝑗) 𝐶𝐶𝐿𝐿𝐶𝐶𝐶𝐶𝑖𝑖,𝑖𝑖+1)𝑛𝑛\\n𝑖𝑖=1(9) \\nwhere 𝑅𝑅𝑥𝑥 represents the unitary gate of rotation -𝑥𝑥. 𝜃𝜃𝑖𝑖+𝑛𝑛×𝑗𝑗 \\nis adjustable parameter of unitary gates. 𝐶𝐶𝐿𝐿𝐶𝐶𝐶𝐶𝑖𝑖,𝑖𝑖+1 \\nrepresents 𝐶𝐶𝐿𝐿𝐶𝐶𝐶𝐶 gate with 𝑚𝑚 as the control qubit, and 𝑛𝑛 is \\nthe total number of qubits. Figure 10 show the basic \\nquantum circuit with n=4 and m=3. Each qubit uses the \\nrotation gate 𝑅𝑅(𝜃𝜃) by the angle 𝜃𝜃 around x-axis on the \\nHilbert space , and  𝐶𝐶𝐿𝐿𝐶𝐶𝐶𝐶 gate is used for every 2 qubits.  \\n3.3. Fully Connection Layer  \\nAfter obtaining all features from PQC, we feed them into \\na fully connection (FC) layer. We use the softmax activation function, so the output of FC  layer will be a \\nprobability distribution. The i -th element of the output is \\nthe probability that this data point belongs to the i -th \\ncategory, and we predict that this data point belongs to the \\ncategory with the highest probability. In order to predict the actual label, we calculate the cumulative dis tance'),\n",
       " Document(metadata={'source': 'data/Yang_Semiconductor_Defect_Detection_by_Hybrid_Classical-Quantum_Deep_Learning_CVPR_2022_paper.pdf', 'page': 6}, page_content='where 𝜔𝜔1,𝑏𝑏1 is the parameter of classical layer. 𝜃𝜃 is the \\nparameter of quantum layer. 𝜔𝜔2,𝑏𝑏2 is the parameter of FC \\nlayer. Perform the above procedure enough times to get a \\ngood estimate of the expected value of 𝑦𝑦�𝑗𝑗𝑖𝑖 with minimum \\nloss function.  \\n4. Results  \\n4.1. Experiment C ircuits \\nIn deep learning, it is very useful to transform data into \\na higher -dimensional feature space. In the same way, there \\nare two strategies for using quantum circuits to generate \\nhigher dimensional features: entangling more and more \\nqubits. However, quantum computers are now in their \\ninfancy, the available qubits are limited. In this experiment, \\nwe adopt different entangling strategies to verify our \\nmethod on the 4 qubits circuit s ystem.  In addition to the \\nbasic quantum circuit (as shown in Figure -10), we also \\nchose Circuit -5/6/16/17 provided by Sim et al. [29], which \\nhas better expressive ability and entanglement ability, as the parametrized quantum circuit (as shown in Figure 11).  \\nTable 1: Ablation Study of Defect Pattern Classification  \\n4.2. Ablation Study  \\nTo demonstrate the usefulness of  quantum circuit, our \\nmodel had been verified on two industry data sets, defect pattern and EPI hotspot. We performed a rigorous ablation study and showed the quantitative comparison in Table 1. \\nWe refer to ResNet50, SENet, MobileNetV3 and our \\nproposed model as the basic model to integrate with \\nvarious quantum circuits. In addition to the basic circuit we proposed, we added the Circuit -5/6/16/17 presented in the \\nprevious section. We also adopted four encoding strategy: \\nbasic, amplitude, and angle encoding.  \\nAblation results yielded many significant findings. First, \\nCircuit 5 and Circuit 6 is a fully connected graph Model  Classical \\nLayer  Quantum Layer  \\nFeature  \\nExtraction  Encoding  Circuit  \\nBasic  5 6 16 17 \\nHybrid  ResNet50  \\n[27] Basic  93.73  95.42  95.03  94.42  93.94  \\nAmplitude  93.74  95.43  95.04  94.43  93.95  \\nAngle  93.81  95.50  95.11  94.51  94.02  \\nClassical  - 92.76  \\nHybrid  SENet  \\n[45] Basic  95.09  96.75  96.43  96.04  95.85  \\nAmplitude  95.27  96.94  96.61  96.22  96.03  \\nAngle  95.42  97.08  96.75  96.36  96.17  \\nClassical  - 93.55  \\nHybrid  MobileNetV3  \\n[31] Basic  92.93  94.13  93.70  93.24  93.40  \\nAmplitude  93.02  94.21  93.79  93.33  93.49  \\nAngle  93.39  94.58  94.15  93.70  93.86  \\nClassical  - 92.09  \\nHybrid  SP&A -Net \\n(proposed \\nmodel)  Basic  95.92  97.85  97.05  96.54  96.11  \\nAmplitude  96.06  97.99  97.19  96.68  96.25  \\nAngle  96.55  98.47  97.68  97.17  96.73  \\nClassical  - 94.27   \\nFigure 11: Quantum Circuit by Sim et al. [ 29]. \\n \\nFigure 12: ResNet50 [27] with Different S trategy  of Quantum \\nCircuit for Defect Pattern Classification  \\n \\nFigure 13: SENet [ 45] with Different S trategy  of Quantum \\nCircuit for EPI Hotspot Classification  \\n2329'),\n",
       " Document(metadata={'source': 'data/Yang_Semiconductor_Defect_Detection_by_Hybrid_Classical-Quantum_Deep_Learning_CVPR_2022_paper.pdf', 'page': 9}, page_content='Biased Learning ,\" Design Automation Conference (DAC) , \\n2017.  \\n[33] H. Yang, L. Luo, J. Su, C. Lin and B. Yu, \"Imbalance aware \\nlithography hotspot detection: a deep learning approach,\" \\nSPIE Advanced Lithography , 2017.  \\n[34] Y. Tomioka, T. Matsunawa, C. Kodama and S. Nojima, \"Lithograp hy hotspot detection by two-stage cascade \\nclassifier using histogram of oriented lieht propagation,\" 22nd ASP -DAC , 2017.  \\n[35] F. Yang, C. C. Chiang, X. Zeng and D. Zhou, \"Efficient \\nSVM- based hotspot detection using spectral clustering,\" \\n2017 IEEE International Symposium on Circuits and Systems (ISCAS) , pp. 1 -4, 2017.  \\n[36] V. Borisov and J. Scheible, \"Lithography Hotspots \\nDetection Using Deep Learning,\" 2018 15th International \\nConference on Synthesis, Modeling, Analysis and \\nSimulation Methods and Applications to Circuit Design (SMACD) , pp. 145-148, 2018.  \\n[37] H. Yang, Y. Lin, B. Yu and E. F. Y. Young, \"Lithography \\nhotspot detection: From shallow to deep learning,\" 2017 \\n30th IEEE International System -on-Chip Conference \\n(SOCC), pp. 233-238, 2017.  \\n[38] Xuezhong Lin, Jingyu Pan, Jinming Xu, Yiran Chen, Cheng Zhuo, \"Lithography Hotspot Detection via Heterogeneous \\nFederated Learning with Local Adaptation,\"  arXiv , Jul. 2021.  \\n[39] J. Yu, X. Zheng and J. Liu, \"Stacked convolutional sparse denoising auto-encoder for identification of defec t patterns \\nin semiconductor wafer map,\" Comput. Ind. , vol. 109, pp. \\n121-133, Aug. 2019.  \\n[40] J. Yu, \"Enhanced stacked denoising autoencoder -based \\nfeature learning for recognition of wafer map defects,\" in \\nIEEE Transactions on Semiconductor Manufacturing , vol. \\n32, no. 4, pp. 613 -624, Nov. 2019.  \\n[41] S. Kang, \"Rotation -invariant wafer map pattern \\nclassification with convolutional neural networks,\" IEEE \\nAccess , vol. 8, pp. 170650-170658, 2020.  \\n[42] U. Batool, M. I. Shapiai, H. Fauzi and J. X. Fong, \\n\"Convolutional neural netw ork for imbalanced data \\nclassification of silicon wafer defects,\" Proc. 16th IEEE Int. \\nColloq. Signal Process. Appl. (CSPA) , pp. 230 -235, Feb. \\n2020.  \\n[43] M. Saqlain, Q. Abbas and J. Y. Lee, \"A deep convolutional \\nneural network for wafer defect identification on an \\nimbalanced dataset in semiconductor manufacturing \\nprocesses,\" in IEEE Transactions on Semiconductor \\nManufacturing, vol. 33, no. 3, pp. 436-444, Aug. 2020.  \\n[44] D. Kim and P. Kang, \"Dynamic Clustering for Wafer Map \\nPatterns using Self -Supervised Learning on Convolutional \\nAutoencoders,\" in IEEE Transactions on Semiconductor \\nManufacturing, 2021.  \\n[45] J. Hu, L. Shen and G. Sun, “Squeeze-and -Excitation \\nNetworks,” 2018 IEEE /CVF Conference on Computer \\nVision and Pattern Recognition (CVPR) , pp. 7132-7141, Jun. \\n2018.  \\n2332')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriver = vectorstore.as_retriever()\n",
    "\n",
    "retriver.invoke(\"self-proliferation block\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The Self-Proliferation Block (SP&A Block) is a new network architecture proposed in the paper that can perform feature engineering more efficiently than traditional deep learning architectures. The SP&A Block consists of multiple layers, each of which performs a series of linear transformations on the input data, followed by a self-attention mechanism that learns long-range dependencies from the generated feature maps.\\n\\nThe self-proliferation block is designed to take advantage of the concept of \"self-proliferation,\" which means that each layer in the block is fed back into itself multiple times, allowing the network to learn more complex and abstract representations of the input data. This can help to improve the efficiency and accuracy of the feature engineering process.\\n\\nIn contrast to traditional deep learning architectures, which often rely on a fixed number of layers and a predefined set of activation functions, the SP&A Block is highly modular and adaptable, allowing it to be easily customized and optimized for different tasks and datasets.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "chain = (\n",
    "    {\"context\": itemgetter(\"question\") | retriver, \"question\": itemgetter(\"question\")}\n",
    "    | prompt\n",
    "    | model\n",
    ")\n",
    "\n",
    "chain.invoke({\"question\": \"What is self-proliferation block?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What is self-proliferation block?'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "itemgetter(\"question\")({\"question\":\"What is self-proliferation block?\"}) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The self-proliferation block is a proposed architecture in the paper you provided, which combines multiple linear transformations to generate more feature maps at a cheaper computing cost. This block is designed to perform feature engineering in a more efficient way than traditional deep learning architectures.\n",
      "\n",
      "In traditional deep learning, each layer is composed of a series of linear transformations followed by an non-linear activation function. However, this can lead to a computationally expensive process, especially when dealing with large datasets. The self-proliferation block addresses this issue by applying multiple linear transformations in parallel, resulting in a more efficient feature extraction process.\n",
      "\n",
      "The self-proliferation block consists of several linear transformations, each of which takes the output of the previous layer and applies additional linear transformations to generate new feature maps. This process is repeated multiple times within the block, allowing the model to learn increasingly complex features from the input data. By combining these linear transformations in a single block, the self-proliferation block can learn richer representations of the input data than traditional deep learning architectures.\n",
      "\n",
      "The self-proliferation block is used in combination with the self-attention block in the proposed HCQDL model to perform feature engineering and defect detection in semiconductor manufacturing. The self-attention block learns a wealth of information about long-range dependencies in the input data, which can be used to improve the accuracy of defect detection.\n"
     ]
    }
   ],
   "source": [
    "print(chain.invoke({\"question\": \"What is self-proliferation block?\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
